import re
import nltk
import spacy
from typing import List, Dict
from collections import Counter

# Download necess√°rio para NLTK
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

class NLPAnalyzer:
    """Classe para analisar problemas de PLN em texto transcrito"""
    
    def __init__(self):
        # Carregar modelo do spaCy para ingl√™s
        try:
            self.nlp = spacy.load("en_core_web_sm")
        except OSError:
            print("‚ö†Ô∏è Modelo spaCy n√£o encontrado. Execute: python -m spacy download en_core_web_sm")
            self.nlp = None
        
        # Dicion√°rio de hom√≥fonos comuns em ingl√™s
        self.homophones = {
            'to': ['too', 'two'],
            'too': ['to', 'two'],
            'two': ['to', 'too'],
            'there': ['their', 'they\'re'],
            'their': ['there', 'they\'re'],
            'they\'re': ['there', 'their'],
            'your': ['you\'re'],
            'you\'re': ['your'],
            'its': ['it\'s'],
            'it\'s': ['its'],
            'hear': ['here'],
            'here': ['hear'],
            'write': ['right', 'rite'],
            'right': ['write', 'rite'],
            'rite': ['write', 'right'],
            'new': ['knew'],
            'knew': ['new'],
            'four': ['for', 'fore'],
            'for': ['four', 'fore'],
            'fore': ['four', 'for'],
            'eight': ['ate'],
            'ate': ['eight'],
            'one': ['won'],
            'won': ['one'],
            'sun': ['son'],
            'son': ['sun'],
            'flower': ['flour'],
            'flour': ['flower'],
            'break': ['brake'],
            'brake': ['break'],
            'pears': ['pairs'],
            'pairs': ['pears'],
            'pair': ['pear'],
            'pear': ['pair'],
            'sea': ['see'],
            'see': ['sea'],
            'meat': ['meet'],
            'meet': ['meat'],
            'peace': ['piece'],
            'piece': ['peace'],
            'no': ['know'],
            'know': ['no'],
            'by': ['buy', 'bye'],
            'buy': ['by', 'bye'],
            'bye': ['by', 'buy'],
            'read': ['red'],
            'red': ['read'],
            'tail': ['tale'],
            'tale': ['tail'],
            'mail': ['male'],
            'male': ['mail'],
            'sail': ['sale'],
            'sale': ['sail'],
            'wait': ['weight'],
            'weight': ['wait'],
            'weak': ['week'],
            'week': ['weak'],
            'steel': ['steal'],
            'steal': ['steel'],
            'bear': ['bare'],
            'bare': ['bear'],
            'fair': ['fare'],
            'fare': ['fair']
        }
        
        # Frases problem√°ticas conhecidas para segmenta√ß√£o
        self.segmentation_patterns = [
            # Padr√£o: texto sem v√≠rgula que muda o sentido
            r"let'?s eat \w+",  # "let's eat grandma" vs "let's eat, grandma"
            r"come and eat \w+",
            r"time to eat \w+",
        ]
    
    def analyze_speech_text(self, text: str) -> Dict:
        """
        Analisa texto de fala para problemas de PLN
        
        Args:
            text: Texto transcrito do reconhecimento de fala
            
        Returns:
            Dicion√°rio com an√°lise dos problemas encontrados
        """
        problems = {
            'original_text': text,
            'homophones': self._find_homophones(text),
            'punctuation': self._check_punctuation_problems(text),
            'segmentation': self._check_segmentation(text)
        }
        return problems
    
    def display_detailed_analysis(self, problems: Dict):
        """Exibe an√°lise detalhada dos problemas encontrados"""
        text = problems['original_text']
        
        print(f"\n{'='*60}")
        print(f"üìù TEXTO TRANSCRITO: '{text}'")
        print(f"{'='*60}")
        
        total_problems = 0
        
        # Hom√≥fonos
        if problems['homophones']:
            print("\nüîÑ HOM√ìFONOS DETECTADOS:")
            for h in problems['homophones']:
                print(f"   ‚ö†Ô∏è  Palavra: '{h['word']}'")
                print(f"   ü§î Pode ser confundida com: {', '.join(h['alternatives'])}")
                total_problems += 1
        
        # Segmenta√ß√£o (v√≠rgulas)
        if problems['segmentation']['has_problems']:
            print("\nüìñ PROBLEMAS DE SEGMENTA√á√ÉO:")
            for issue in problems['segmentation']['issues']:
                print(f"   ‚ö†Ô∏è  {issue}")
                total_problems += 1
        
        # Pontua√ß√£o geral
        if problems['punctuation']['has_problems']:
            print("\nüìù OUTROS PROBLEMAS DE PONTUA√á√ÉO:")
            for issue in problems['punctuation']['issues']:
                print(f"   ‚ö†Ô∏è  {issue}")
                total_problems += 1
        
        # Resumo
        if total_problems == 0:
            print("\n‚úÖ NENHUM PROBLEMA DETECTADO!")
        else:
            print(f"\nüìä RESUMO: {total_problems} problema(s) detectado(s)")
        
        print(f"{'='*60}\n")
    
    def _find_homophones(self, text: str) -> List[Dict]:
        """Detecta hom√≥fonos problem√°ticos"""
        words = re.findall(r'\w+', text.lower())
        found_homophones = []
        
        for word in words:
            if word in self.homophones:
                alternatives = self.homophones[word]
                found_homophones.append({
                    'word': word,
                    'alternatives': alternatives,
                    'context': text
                })
        
        return found_homophones
    
    def _check_segmentation(self, text: str) -> Dict:
        """Verifica problemas de segmenta√ß√£o de palavras"""
        issues = []
        text_lower = text.lower()
        
        # Verificar padr√µes problem√°ticos espec√≠ficos
        for pattern in self.segmentation_patterns:
            if re.search(pattern, text_lower):
                if "eat" in text_lower:
                    issues.append("Frase amb√≠gua detectada - v√≠rgula ausente pode mudar sentido (ex: 'eat grandma' vs 'eat, grandma')")
        
        # Palavras muito longas (poss√≠vel jun√ß√£o)
        words = text.split()
        long_words = [w for w in words if len(re.sub(r'[^\w]', '', w)) > 15]
        if long_words:
            issues.append(f"Palavras muito longas: {', '.join(long_words)}")
        
        return {
            'issues': issues,
            'has_problems': len(issues) > 0
        }
    
    def _check_punctuation_problems(self, text: str) -> Dict:
        """Verifica problemas gerais de pontua√ß√£o"""
        issues = []
        
        # Texto sem pontua√ß√£o final
        if not re.search(r'[.!?]$', text.strip()):
            issues.append("Frase sem pontua√ß√£o final")
        
        # M√∫ltiplas frases sem separa√ß√£o
        if len(text.split()) > 10 and not re.search(r'[.!?,;]', text):
            issues.append("Texto longo sem pontua√ß√£o interna")
        
        return {
            'issues': issues,
            'has_problems': len(issues) > 0
        }
    
    # Mant√©m m√©todos originais para compatibilidade
    def identify_problems(self, text: str) -> List[str]:
        """
        Identifica problemas potenciais para PLN no texto transcrito
        
        Args:
            text: Texto transcrito do reconhecimento de fala
            
        Returns:
            Lista de problemas identificados
        """
        problems = []
        
        # 1. HOM√ìFONOS - Verificar ambiguidade lexical
        homophones_found = self._find_potential_homophones(text)
        if homophones_found:
            problems.append(f"üî§ AMBIGUIDADE LEXICAL: Hom√≥fonos detectados: {', '.join(homophones_found)} (podem ter significados diferentes)")
        
        # 2. SEGMENTA√á√ÉO - Verificar problemas de pontua√ß√£o cr√≠ticos
        segmentation_issues = self._find_segmentation_problems(text)
        if segmentation_issues:
            problems.append(f"üìù ERRO DE SEGMENTA√á√ÉO: {segmentation_issues}")
        
        # 3. Verificar aus√™ncia geral de pontua√ß√£o
        if self._lacks_punctuation(text):
            problems.append("‚ö†Ô∏è PONTUA√á√ÉO AUSENTE: Falta de pontua√ß√£o pode alterar significado")
        
        return problems
    
    def _find_segmentation_problems(self, text: str) -> str:
        """Identifica problemas espec√≠ficos de segmenta√ß√£o que mudam o sentido"""
        text_lower = text.lower()
        
        # Verificar padr√µes problem√°ticos espec√≠ficos
        for pattern in self.segmentation_patterns:
            if re.search(pattern, text_lower):
                if "eat" in text_lower:
                    return "Frase amb√≠gua detectada - v√≠rgula ausente pode mudar sentido (ex: 'eat grandma' vs 'eat, grandma')"
        
        # Verificar frases longas sem pontua√ß√£o
        words = text.split()
        if len(words) > 8 and not re.search(r'[,.!?;:]', text):
            return "Frase longa sem pontua√ß√£o - dificulta an√°lise sint√°tica"
        
        return None
    
    def _lacks_punctuation(self, text: str) -> bool:
        """Verifica se o texto carece de pontua√ß√£o adequada"""
        # Textos longos sem pontua√ß√£o s√£o problem√°ticos
        words = text.split()
        punctuation_marks = ['.', '!', '?', ',', ';', ':']
        
        if len(words) > 10:  # Frases com mais de 10 palavras
            has_punctuation = any(mark in text for mark in punctuation_marks)
            return not has_punctuation
        
        return False
    
    def _find_potential_homophones(self, text: str) -> List[str]:
        """Encontra hom√≥fonos potenciais no texto"""
        words = text.lower().split()
        found_homophones = []
        
        for word in words:
            # Remove pontua√ß√£o
            clean_word = re.sub(r'[^\w]', '', word)
            if clean_word in self.homophones:
                found_homophones.append(clean_word)
        
        return list(set(found_homophones))
    
    def get_detailed_analysis(self, text: str) -> Dict:
        """Retorna an√°lise detalhada do texto"""
        if not self.nlp:
            return {"error": "spaCy model not available"}
        
        doc = self.nlp(text)
        
        analysis = {
            "tokens": [token.text for token in doc],
            "pos_tags": [(token.text, token.pos_) for token in doc],
            "entities": [(ent.text, ent.label_) for ent in doc.ents],
            "sentences": [sent.text for sent in doc.sents]
        }
        
        return analysis